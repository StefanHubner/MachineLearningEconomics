The dappled world
Speech given by
Andrew G Haldane, Chief Economist, Bank of England

GLS Shackle Biennial Memorial Lecture
10 November 2016

The views are not necessarily those of the Bank of England or the Monetary Policy Committee.
I would like to thank Arthur Turrell for his help in preparing the text and Rafa Baptista, Karen Braun-Munzinger,
Angelina Carvalho, Doyne Farmer, Jeremy Franklin, Marc Hinterschweiger, Andreas Joseph, Katie Low, Zijun Liu,
Matthew Manning, Rajan Patel, Paul Robinson, Daniel Tang, Ryland Thomas and Arzu Uluc for their comments and
contributions.

1

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

Introduction
I am delighted to be giving this year’s Shackle Biennial Memorial Lecture.

The past few years have witnessed a jarring financial crisis as great as any have experienced since the
world wars, a crisis whose aftershocks are still being felt today. Against that dramatic backdrop, I thought I
would use this occasion to reflect on the state of economics, not least in helping us make sense of such
catastrophic phenomena.

This topic has risen in both prominence and urgency since the financial crisis (Coyle and Haldane (2015),
Battiston et al (2016)). Indeed, it would probably not be an exaggeration to say the economic and financial
crisis has spawned a crisis in the economics and finance profession - and not for the first time. Much the
same occurred after the Great Depression of the 1930s when economics was rethought under Keynes’
intellectual leadership (Keynes (1936)).
Although this crisis in economics is a threat for some, for others it is an opportunity – an opportunity to make
a great leap forward, as Keynes did in the 1930s. For the students in this room, there is the chance to
rethink economics with as clean a sheet of paper as you are ever likely to find. That is perhaps why the
numbers of students applying to study economics has shot up over recent years. This is one of the silver
linings of the crisis. No discipline could ask for a better endowment.

But seizing this opportunity requires a re-examination of the contours of economics and an exploration of
some new pathways. That is what I wish to do in this lecture, drawing liberally on the work of
George Shackle. In the light of the crisis, there has been renewed interest in Shackle’s work as economists
have sought new insights into age-old problems. Indeed, it was this quest that first brought Shackle’s work
to my own attention around five years ago.

In exploring new pathways, I will draw my inspiration from three features of economic systems which
underpinned Shackle’s own work. First is the importance of recognising that these systems may often find
themselves in a state of near-continuous disequilibrium. Indeed, even the notion of an equilibrium, stationary
through time, may itself be misleading (Shackle (1972)). It was perhaps this feature of Shackle’s work that
earned him his heterodox label.

Second is the importance of looking at economic systems through a cross-disciplinary lens. Drawing on
insights from a range of disciplines, natural as well as social sciences, can provide a different perspective on
individual behaviour and system-wide dynamics. In Shackle’s own work he drew liberally on a range of
economic schools of thought, from Keynesian to Austrian, as well as history, philosophy and psychology
(Shackle (1949, 1979)).

2

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

2

Third is the importance of radical uncertainty. Shackle saw uncertainty about the future as fundamental to
human decision-making and, thus, to the functioning of social systems (Shackle (1972)). Human imagination
was a crucial frame for social progress (Shackle (1979)). This meant social systems were inherently
unpredictable in their behaviour. Latterly, the importance of radical uncertainty in making sense of social
systems has gained new traction (Taleb (2014), King (2016)).

I wish to explore whether there are insights and techniques from other disciplines which better capture these
features of economic systems and which thus hold promise when moving the economics profession forward.
These approaches would still be considered heterodox in much of the profession even though, in many other
fields, they would be considered entirely standard.

One of the potential failings of the economics profession, I will argue, is that it may have borrowed too little
from other disciplines - a methodological mono-culture. In keeping with this spirit, the title of my lecture is
itself borrowed. In 1999 Professor Nancy Cartwright, a philosopher of science, published a book with the title
The Dappled World: A Study of the Boundaries of Science (Cartwright (1999)). This quote captures its
essence:
“Science as we know it is apportioned into disciplines, apparently arbitrarily grown up; governing
different sets of properties at different levels of abstraction; pockets of great precision; large parcels
of qualitative maxims resisting precise formulation; erratic overlaps; here and there, once in a while,
corners line up but mostly ragged edges; and always the cover of law just loosely attracted to the
jumbled world of material things.”

Cartwright was describing the natural sciences. She describes them as a patchwork of theory and evidence,
some of it precisely cut, most of it haphazardly shaped and pieced together irregularly. And there is, she
argues, no shame in that. To the contrary, it may be the best science can do, given limited knowledge and
limited time, in trying to make sense of an often-jumbled world.
Here is another way of putting Cartwright’s point. Sometimes the popular perception of the natural sciences,
and physics in particular, is that it too is a methodological monoculture. The defining characteristics of this
monoculture are great laws and unifying theories. For example, it is perhaps no accident that the world’s
2

most famous equation is this one: E= MC . Not only is this uniquely famous equation drawn from theoretical
physics; it defines a great law, a unifying theory.

Yet much of modern physics does not, in fact, deal in great laws and unifying theories. Instead it deals in
great unknowns and empiric regularities, in ragged edges and qualitative maxims (Buchanan (2014)).
Advances in physics these days often come courtesy of massive empirical fishing expeditions,

3

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

3

industrial-scale searches for needles in haystacks, often using large-scale computational techniques. The
large Hadron collider – in essence, a massive Monte Carlo machine - would be one prominent example of
these techniques in practice.

So even physics, for some the theoretical pinnacle of the natural sciences, is these days far from being a
precise science. Through its evolution, the intellectual bloodline of physics and the other natural sciences
has been mixed and re-mixed. What we have today is a methodological hybrid, an intellectual mongrel. This
has been not just a natural evolution, but an essential one in making sense of our dappled world.

From Natural to Social Sciences
While Cartwright’s Dappled World aims to dispel the notion that natural science is precise science, it goes
further in arguing that the self-same case can be made, just as forcefully, for the social sciences. It, too, is a
haphazard patchwork of theories and hunches, operating at different levels of abstraction, often loosely held
together. The economic and financial world is every bit as dappled as the natural world. And, as with the
natural sciences, there is real virtue in that eclectic approach.

Yet this view jars somewhat with the dominant methodological direction of travel in economics. That
methodological lead was provided by Karl Popper in the 1930s (Popper (1934)). In The Logic of Scientific
Discovery, Popper argued for a “deductive” approach to advancing knowledge. This involved, first,
specifying a clear set of assumptions or axioms. From those were deduced a set of logical propositions or
hypotheses. And then, and only then, were these hypotheses taken to the data to be validated or falsified.

This pursuit of empirically-falsifiable hypotheses had even-earlier antecedents. The use of dynamic
optimisation techniques goes back at least to Leon Walras (Walras (1874)). And he, in turn, drew on the
tools developed by mathematicians such as Leibniz, Lagrange and Euler. These techniques were used to
construct early “general equilibrium” models of the economy, models which where internally consistent when
looked at as a whole.

Newtonian physics was built on the same principle of internal consistency within systems, with energy within
that system always preserved. Those systems were subject to disturbances which could cause them to
oscillate dynamically. Ultimately, however, these systems typically had an equilibrium or steady-state to
which they returned. As one of the simplest examples, Newton’s pendulum exhibits damped harmonic
motion once displaced before returning to a state of rest.

Much of mainstream macro-economics and finance has essentially followed this intellectual lead. It typically
starts with a set of assumptions or axioms – in economics often defining the preferences of consumers and
the technology facing firms. From those assumptions are derived equations of motion for the behaviour of

4

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

4

consumers and firms – which, rather revealingly, are often called the Euler conditions. Then, and only then,
are these first-order conditions for behaviour taken to the data to be tested.
These models, so derived, predictably exhibit the same damped harmonic motion as Newton’s pendulum.
Let me give a couple of examples of this approach in mainstream practice in economics and finance. The
most famous equation in finance is probably this one:

Figure 1: The Black-Scholes Formula

Source: Black and Scholes (1973).

This is the Black-Scholes options pricing formula (Black and Scholes (1973)). Under a (relatively) small set
of assumptions, it provides a (relatively) simple analytical description of how to price a financial option. It
cannot rival Einstein for its simplicity and aesthetic beauty. But it nonetheless has many of the trademark
characteristics of theoretical physics.

That should come as no surprise because this equation was itself drawn from theoretical physics. In seeking
a solution to their option-pricing problem, Fisher Black, Myron Scholes and Robert Merton drew an explicit
link between their contingent-claims pricing problem and the heat transfer equation in physics (Churchill
(1963)). If not quite a lift and shift from theoretical physics, the Black-Scholes formula was certainly a
genetic mutation.

Moving from finance to economics, the dominant approach over recent decades to modelling the
macro-economy has probably been the Dynamic Stochastic General Equilibrium (DSGE) model (Smets and
Wouters (2003)). In its plain-vanilla form, this comprises a set of representative, optimising households and
firms. This model gives rise to an equilibrium which is unique and stationary, and dynamics around that
equilibrium which are regular and oscillatory.

Various knobs and whistles have been added to this workhorse framework, often involving market frictions in
price-setting, competition and credit provision. These add colour to the model’s dynamics but, by and large,
leave intact its properties – stable, stationary, oscillatory. It is not just among academics that this workhorse
5

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

5

framework has found favour. The majority of central banks also take the DSGE framework as their starting
point, including the Bank of England (Burgess et al (2013)).

The DSGE approach has many of the hallmarks of Newtonian physics. As every action has an equal and
opposite reaction, every shock has an equal and proportional reaction in DSGE models. The economy’s
dynamics exhibit the same damped harmonic motion as Newton’s pendulum, or if a rocking horse were to be
hit with a stick. The rocking horse metaphor is apt, as it was first used by Swedish economist Knut Wicksell
almost a hundred years ago to describe the business cycle motion of an economy (Wicksell (1918)).

1

Mainstream finance and macroeconomics has, then, followed firmly in the footsteps of giants, part
Popperian, part Newtonian. It has been heavily indebted, intellectually, to Classical physics. That has led
some to dub the dominant economic paradigm “econo-physics” (Mirowski (1989)). Less kindly, some have
described economics as suffering from physics-envy.

Assessing the Pros and Cons

Despite recent criticism, which has come thick and fast, it is important not to overlook the benefits from
having followed this path. One benefit, shared with theoretical physics, is that economic theory has
well-defined foundations. There are fewer “free”, or undefined, parameters floating around the model. Nobel
2

Laureate Robert Lucas said “beware of economists bearing free parameters”. He was right. A theory of
everything is a theory of nothing.
The advantages do not stop there. On the assumption agents’ behaviour is representative – it broadly
mirrors the average person’s – these models of micro-level behaviour can be simply-summed to replicate
macro-economic behaviour. The individual is, in effect, a shrunken replica of the economy as a whole.
These macro-economic models are, in the jargon, micro-founded – that is, constructed bottom-up from
optimising, micro-economic foundations.

These advantages carry across into the policy sphere. If the assumptions underlying these models are valid,
then the behavioural rules from which they are derived will be unaffected by changes in the prevailing policy
regime. These models are then a robust test-bed for policy analysis. They are, in economists’ jargon,
immune to the Lucas critique (Lucas (1976)). This feature, above all others, probably explains these models’
ubiquity in policy organisations.

Not least in the light of the crisis, however, the potential pitfalls of these approaches have also become
clearer of late. Recently, these models have been subject to stinging critiques (Romer (2016)). One

1

“If you hit a rocking horse with a stick, the movement of the horse will be very different from that of the stick. The hits are the cause of
the movement, but the system’s own equilibrium laws condition the form of movement” (Wicksell (1918)).
2
Attributed to Robert Lucas in Sargent (2001).
6

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

6

common complaint is that they may not do an especially good job of describing the real world, especially in
situations of economic stress. Exhibit one is that they offered a spectacularly poor guide to the economy’s
dynamics around the time of the global financial crisis.

To illustrate, Chart 1 plots the range of forecasts for UK GDP growth from 2008 onwards produced by 27
economic forecasters (including the Bank) in 2007, the dawn of the financial crisis. Three features are
notable. First, pre-crisis forecasts were very tightly bunched in a range of one percentage point, perhaps
because they were drawn from similar models. The methodological mono-culture produced, unsurprisingly,
the same crop.

Second, these forecasts foresaw a continuation of the gentle undulations in the economy seen in the decade
prior to the crisis - the so-called Great Moderation (Bernanke (2004)). At the time, these damped oscillations
seemed to match well the damped harmonic motion of DSGE models. A good crop today foretold of an only
slightly less good crop tomorrow.

Third, most striking of all, every one of these forecasts was not just wrong but spectacularly so. Few
forecasters foresaw even a slight downturn in GDP in 2008 and none foresaw a recession. Yet we
witnessed not just a recession but the largest since the 1930s. The one-year-ahead forecast error in 2008
was 8 percentage points. The crop failed and the result was economic famine.

While forecasting performance has improved in the period since, there has a continued string of serially
correlated errors, with the speed of the recovery consistently over-estimated (Chart 2). The average forecast
error one-year-ahead has been consistently negative, averaging 0.5 percentage points per year. The
average error two years ahead has been over one percentage point per year.

At root, these were failures of models, methodologies and mono-cultures. It has been argued that these
models were not designed to explain such extreme events. To quote Robert Lucas once more: “The charge
is that the [..] forecasting model failed to predict the events of September 2008. Yet the simulations were not
presented as assurance that no crisis would occur, but as a forecast of what could be expected conditional
on a crisis not occurring” (Lucas (2009)).

7

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

7

Chart 1: Range of GDP forecasts in 2007Q4

Chart 2: Forecasts for World Growth by the IMF since
2007

Latest vintage of GDP data

Year on year growth
4
3

Apr-07
Oct-08
Apr-10
Oct-11
Apr-13

Oct-07
Apr-09
Oct-10
Apr-12
Oct-13

Apr-08
Oct-09
Apr-11
Oct-12
Apr-14

Percent year
on year
7
6

2
November 2007 IR

5
1
4
0

3

-1

2

-2

1

-3

0

-4

-1

-5
2002 2003 2004 2005 2006 2007 2008 2009 2010 2011
Source: Bank of England November 2007 Inflation Report; Bank
of England Survey of Economic Forecasters.

-2
2000 2002

2004 2006

2008

2010

2012 2014

2016

Source: IMF World Economic Outlook.

For me, this is not really a defence. Economics is important because of the social costs of extreme events.
Economic policy matters precisely because of these events. If our models are silent about these events, this
jeopardises the very thing that makes economics interesting and economic policy important. It risks
squandering the human capital endowment the financial crisis has bestowed.

Even if some of the post-crisis criticism of workhorse macro-economic models is overdone, it still raises the
question of whether new modelling approaches might be explored which provide a different lens on the world
or which better match real-world dynamics. If, as Cartwright suggests, there is more methodologically that
links the natural and social sciences than divides them, there could be considerable scope for disciplinary
cross-pollination of ideas and models.
So just how interdisciplinary is economics? The short answer appears, regrettably, to be “not very”. Table 1
looks at responses by professionals in different social sciences to the proposition “inter-disciplinary
knowledge is better than knowledge obtained by a single discipline” (Fourcade, Ollion and Algan (2015)). A
majority, often a large majority, of other social sciences agree with this statement. Only in economics do a
majority disagree.

8

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

8

Table 1: Responses to the question - Interdisciplinary knowledge is better than knowledge obtained by a
single discipline?

Source: Fourcade, Ollion and Algan (2015). Notes: Table shows responses as a percentage of professors in each discipline.

Economists’ words are matched by their actions. The science periodical Nature recently published
interdisciplinary indices, measuring the number of references made to outside disciplines by a subject area
and the number of citations made outside the discipline to that subject area. Over the period 1950-2014,
economics sat in the bottom left-hand quadrant (Chart 3). Alongside theoretical physics, economics was at
the bottom of the inter-disciplinary league table (van Noordan (2015)).

Chart 3: Citations and references to outside

Chart 4: Citations and references to outside

disciplines

disciplines over time

Source: van Noordan (2015).

Source: van Noordan (2015).

Looking at these patterns over time paints a somewhat more optimistic picture (Chart 4). There is clear
evidence of economics having become more cross-disciplinary during the course of this century. Once
finance is taken out of these estimates, however, economics continues to lag. And alternative metrics, such
as the degree of rigidity in hierarchies and gender diversity, paint economics in a less favourable light
(Fourcade, Ollion and Algan (2015)). Despite progress, it is difficult to escape the conclusion that economics
remains an insular, self-referential discipline.

9

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

9

The economics profession’s intellectual bloodline is purer than in most other disciplines. In some respects, it
is a source of strength that the workhorse economic model is a thoroughbred. That quest for purity also
contributed importantly, however, to this workhorse falling at the first meaningful fence put in its path. This
suggests it is probably as good a time as any to consider mixing the intellectual bloodline, just as physics
and other of the natural sciences have done.

Agent-Based Models (ABMs)

Against this backdrop, what other disciplines and approaches might be productive alternative lines of
enquiry? One potentially promising strand is so-called agent-based modelling. Agent-based models (ABMs)
are interconnected systems of individual “agents” who follow well-defined behavioural rules of thumb.

What makes these models interesting is that these agents are heterogeneous and interactive. In other
words, these models relax one of the key assumptions of the standard model – a single, representative
agent. For social systems, this does not sound too implausible. Humans are social animals. Indeed, the
feature which set humans apart from other
animals is their degree of social interaction

Chart 5: The most highly cited articles on agent-based

(Harari (2015)).

modelling across disciplines

This, on the face of it, modest adaptation
gives rise to some fundamental changes in
model dynamics. Linear, proportional
responses to shocks become the exception;
complex, non-linear responses the rule.
Single, stationary equilibria become the
exception; multiple, evolutionary equilibria the
rule. I discuss these differences in the next
section.

Before doing so, I want to bring to life some of
the uses of ABMs. These models have found
widespread application across a broad array
of disciplines in both the natural and social
sciences. They have been used to address a

Source: Google scholar; Bank calculations. Notes: This was found by
performing a Google Scholar search of the relevant term for “agentbased modelling” in each field and then using the citation count of the
most highly cited paper which could be fairly and solely attributed to each
distinct field. When searching for “Monte Carlo” only papers modelling
distinct subsystems were considered and not those using the technique
of random number generation more generally.

massive array of problems, big and small: from segregating nuts to segregating races, from simulating the
fate of the universe to simulating the fate of a human cell, from military planning to family planning, from
flocking birds to herding (fat) cats.

10

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

10

Yet in economics, ABMs have been used relatively less (Battiston et al (2016)). Chart 5 shows the number
of citations that the most-cited article on agent-based modelling has in different fields. Economics has been
a late and slow-adopter of ABM techniques. In other words, ABMs may be another example of the lack of
cross-disciplinary perspective in economics.

To be clear, ABMs are no panacea for the modelling ills of economics. In discussing them here, the
implication is not that they should replace DSGE models, lock, stock and barrel. Rather, their value comes
from providing a different, complementary, lens through which to make sense of our dappled economic and
financial world, a lens which other disciplines have found useful when understanding their worlds or when
devising policies to improve it.

(a) Early Origins

ABMs grew out of some of the earliest applications of computing at Los Alamos National Laboratory in the
United States (Metropolis (1987)). In 1945, it was this lab that developed the first-ever nuclear weapon –
the fission bomb. The chain-reactions used in these weapons are based on the fissioning of large atoms by
neutrons. Neutron transport involves high-dimension, discrete collisions, with each step probabilistic. This
quickly gives rise to a vast probability tree of possible outcomes.

Enrico Fermi had experimented with a method to solve these sorts of high-dimensional problems in the
1930s, using a small mechanical adding machine. The technique involved generating random numbers and
comparing them with probabilities derived from theory. By doing this for a large number of possible neutron
paths, he painstakingly painted a picture of the transport of neutrons. Fermi had, by hand, constructed the
first agent-based model.

These models were about to get a machine which could do these large-scale computations for them.
Assembled at Los Alamos during World War II was a dream team of the great and good in science, including
John von Neumann, Richard Feynman, Niels Bohr and Robert Oppenheimer. Von Neumann himself offered
use of one of the first-ever computers to perform neutron calculations. That exercise culminated in the
hydrogen bomb. ABMs had found their first real-world application.

11

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

11

Figure 2: John von Neumann

Figure 3: The hydrogen bomb

Source: LANL (public domain). John von
Neumann in the 1940s (image available here).

Source: The Official CTBTO Photostream (CC-BY-2.0). An atmospheric nuclear test
conducted by the U.S. at Enewetak Atoll on 1 November 1952 (image available
here).

(b) Military Planning
By 1947, the use of random numbers in computation had a name which reflected its probabilistic nature –
3

Monte Carlo, home of the casino. In a lecture in 1948, von Neumann described the idea of cellular
automata - artificial systems of cells exhibiting life-like behaviour which evolved according to simple rules of
behaviour (von Neumann et al (1966), Sarkar (2000)). Perhaps the best-known example of cellular
automata came several years later: Conway's “Game of Life” (Conway (1970)).

In 1949, the emerging technique of Monte Carlo analysis was beginning to gain traction. A symposium
organised and sponsored by the RAND Corporation – a spin-off of the US War Department – was the
springboard for developing some of the first military applications of ABMs. What had once been simple
board-and-dice based war games switched to computational ABMs during the 1960s, borrowing heavily on
von Neumann’s cellular automata (Woodcock et al (1988)).

These models have since been used to provide insights in a range of real-world military operations, past and
present. They have been used to understand the dynamics of past battles, such as the operation of German
U-boat battles (Champagne (2003)), Hill et al (2004)). And they have been used, typically secretly, to plan

3

See Metropolis and Ulam (1949). Metropolis reports that he came up with the name, but it was inspired by Ulam's uncle asking for
money by claiming he “just had to go to Monte Carlo''. By 1949, they had written a paper together outlining the potential uses of this
method including a chain-reaction with particles as Monte Carlo agents.

12

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

12

military strategies for current conflicts, with organisations such as the US Naval Research Laboratory and the
US Air Force investing in ABM technology (Manheimer et al (1997)).

Figure 4: WW2 U-Boat Campaigns

Figure 5: War Games

Source: IWM Collections (public domain). Torpedo fired on a merchant ship
(image available here).

Source: GSC Game World (GNU FDL) (Image from
‘Cosacks: European Wars’ (image available here).

The US Air Force has an agent-based model known as Systems Effectiveness Analysis Simulation (SEAS).
The SEAS package models the entire surface of the earth and its satellites. Each agent has a specific
longitude and latitude to a resolution of one millimetre. In order to allow interaction, SEAS agents are
equipped with sensors, communications, and weapons allowing co-operative or competitive behaviour
(Brooks et al (2004)).

Most recently, the work on ABMs in a military context has taken on a new direction. As autonomous vehicles
such as drones are used in war, they add a new dimension to ABM simulations. In a world of human agents,
ABMs could only approximate agent behaviours. In a world of military automata, these simulations come
closer to replicating real-world warfare (Sanchez and Lucas (2002)).

A more benign military application of ABMs has been in the context of war games. Military ABMs were a
forerunner of many of today’s computer war games: Eve Online, World of Warcraft and Command and
Conquer are all popular, sophisticated ABM-based games which mix computer and human controlled agents.

(c) Physical Sciences

As the Large Hadron Collider illustrates, modern physics is heavily computational in certain fields. This
arises in part because even simple systems can quickly exhibit complex behaviours. In general, a system
comprising three bodies undergoing motion via a force cannot be solved analytically – the “three-body
problem”. And there are many problems in physics which involve the interaction of “particles” in ways which
are complex and non-deterministic.

13

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

13

These features have resulted in ABMs being extremely widely used in the physical sciences, at various
different levels of resolution. At the micro scale, some of the earliest applications of ABMs were to study
plasmas – charged particles which constantly interact. Plasmas are the fourth state of matter after solids,
liquids and gases. They are prime ABM candidates because they exhibit rich, non-linear, complex
behaviour.

In 1966, Edward Teller wrote a paper which described how to build a plasma ABM with 32 interacting
particles (Brush, Sahlin and Teller (1966)). Even with “only” 32 interacting agents, this was a formidable
computational challenge. But the advent of parallel and high performance computing since the 1990s has
changed the scale of plasma particle simulations. Simulations today regularly use as many as 10

9

computational agents (Turrell et al (2015)).

These plasma ABMs have found widespread application, micro and macro. At the micro level, they have
been used to help identify and kill cancer cells with pinpoint accuracy, while leaving healthy surrounding cells
unharmed (Bulanov et al (2002)). At the macro level, they have been used to simulate and reproduce the
way stars produce energy, using a nuclear fusion reactor here on Earth, an approach that has the potential
to provide clean energy for billions of years (Turrell (2013)).

Simulating the processes powering stars is modest by comparison with ABMs which aim to model the entire
universe. In 1985, one of the first comprehensive computational models of the universe was published, with
over 32,000 particles representing known matter (Davis et al (1985)). As estimates put the number of
80

protons and neutrons in the Universe at 10 , this ABM was impressively parsimonious (Penrose (1999)).
These models have generated important insights, including the role of “dark matter” in the evolution of
galaxies.

Not all applications of ABMs are, however, large in scale and universal in application. A 1987 paper called
“Why the Brazil Nuts Are on Top” used an ABM with agents of different physical sizes to explain how mixed
nuts segregate over time when shaken, with Brazil nuts generally rising to the surface (Rosato et al (1987)).
This research was not as nuts as it sounds. It has since found real-world applications in industries such as
pharmaceuticals and manufacturing.

14

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

14

Figure 6: Segregation of nuts

Figure 7: The universe

Source: Sae Rpss (CC-BY-SA-3.0). Nuts (image available here).

Source: NASA Headquarters - Greatest Images of NASA
(NASA-HQ-GRIN) (public domain). Spiral galaxy in the
constellation Coma Berenices (image available here).

(d) Operational Research

ABMs have been used to study, and help solve, a range of logistical problems where these involve
coordination across a large number of agents. One example would be crowd dynamics. ABMs have been
used to simulate crowd behaviour at festivals and parades to aid logistics and improve safety (Batty et al
(2003)). They have also been used to simulate crowd dynamics in emergency situations – for example,
simulating people evacuating a building to determine their speed and to help improve safety procedures.

4

ABMs have been used to model and simulate the behaviour of shoppers – for example, at a micro level
simulating flows of people through a shopping centre. At a macro level, they have been used to simulate
how shopping patterns might be affected by the introduction of new stores. For example, what is the impact
of introducing an out-of-town hyper-market on city-centre stores?

5

ABMs have been extensively used for the study of behaviour in transport and utility networks. They have
been used to simulate fluctuations in demand, and the impact of operational and economic events, on the
functioning of energy grids. For example, the EMCAS (Electricity Market Complex Adaptive System) is an
ABM used to simulate the US electricity grid. Its high level of granularity allows it to compute electricity
prices each hour at each location in the transmission network. EMCAS has also been used to simulate the
introduction of new energy sources and technologies.

In transportation systems, ABMs have been used for traffic management, taxi dispatch, traffic signal control
6

and combined rail and road transport planning. Long before the age of Uber, ABMs were being used to
4
5
6

For example, Pelechano, Allbeck, and Badler (2007).
For example, Rauh, Schenk and Schrödl (2012).
For the review see Chen and Cheng (2010)
15

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

15

7

maximise the efficiency of taxi pick-ups. They have been used as a means of undertaking air traffic control
on a decentralised basis, including by the US Federal Aviation Authority. As well as increasing the efficiency
of routing, this decentralised approach can improve aircraft safety by serving as a backstop when
communication with a central air traffic controller is lost.

8

A more recent application of the same technology is autonomous vehicles, such as driverless cars. As with
aircraft, these too need to solve a complex co-ordination problem, taking account not just of the environment
but the behaviour of other agents. As with aircraft, these models can be used to demonstrate the scope for
both efficiency savings (it is estimated each autonomous vehicle can replicate the performance of 11
conventional ones) and safety improvements (it is estimated an autonomous vehicle network might reduce
road accidents by over 90%).

9

Figure 8: Air traffic and transport control

Figure 9: Electricity grids

Source: United States Air Force (public domain). Military air traffic
controllers in a control tower (image available here).

Source: Rept0n1x (CC-BY-SA-3.0). Power line in Cheshire
(image available here).

(e) Biology

Given their cellular structures, ABMs have found a wide range of biological and medical applications. In
biology, an area which was ripe for ABMs was so-called morphogenesis - the biological process that causes

7
8
9

For the taxi example, see Seow and Lee (2010).
For example, Pechoucek and Sislak (2009).
See Fagnant and Kockelman (2014).
16

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

16

an organism to develop shape and pattern. In 1953, Alan Turing had considered this when explaining the
origins of the “dappled” pattern found on Friesian cattle.

Biologists began thinking about constructing an ABM of an entire cell from the late 1990s (Tomita et al
(1999)). In more recent years, ABMs have broken into the biomedical sciences, being applied to cancer,
immunology, vascular disease and in vitro development (Thorne et al (2007)). One of the most exciting
recent developments in ABMs has been the construction of a complete model of the human pathogen
Mycoplasma genitalium (Karr et al (2012)). These models are likely to become more popular with the advent
of powerful gene editing technologies.

Most recently, ABMs has been to streamline drug or device design, simulate clinical trials and predict the
effects of drugs on individuals. ABMs can help bridge the different scales inherent in biological problems –
from gene to cell, from cell to tissue, and from tissue to organism. Concrete applications have included the
treatment of acute inflammation and the healing of wounds (An et al (2009)); the optimal rest periods
necessary to enhance bone formation;
spread.

10

and the distribution, or “hallmarking”, of cancer cells and their

11

In the field of disease control, some recent major work in epidemiology has only been achievable through the
use of ABMs. For example, using geographical data, commuting patterns, age distributions and other
census derived information, an ABM was recently used to simulate the spread of an influenza pandemic
across all 57 million of Italy’s inhabitants (Degli et al. (2008)). This approach could, in principle, be tailored to
other epidemics on a country-by-country basis.

In the treatment of patients, ABMs have been used as a tool for scheduling in Emergency Departments.
Using data on the weekly distribution of patient arrivals and the number of physicians available, ABMs have
been used to determine which combinations of resources reduce the ‘door-to-doc’ time for patients.

10
11
12

12

See Ausk, Gross and Srinivasan (2006).
For more, see Mansury et al. (2002).
See Jones and Evans (2008).
17

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

17

Figure 10: Friesian Cattle

Figure 11: Bio-medical science

Source: Keith Weller/USDA (public domain). Holstein cow
(image available here).

Source: National Institutes of Health (public domain). Medical
Laboratory Scientist (image available here).

(f) Ecology

ABMs have been applied to a number of ecological problems involving the interaction of multiple agents with
each other and with the environment. In the 1990s, ABMs were applied to population and resource
dynamics and migration patterns (Grim (1999)). For example, there is now a large body of work on ABMs of
marine organisms (Werner et al (2001)), simulating how these organisms are moved around by ocean
currents and how they interact in food webs.

Model-derived fish spawning locations have been found to coincide with observed spawning locations of fish
(Heath and Gallego (1998)). These models have also been used to simulate the effects of hypothetical
interventions, such as the impact of introducing new predatory species to eliminate pests, different
techniques for managing fish stocks, or the impact of contamination of marine eco-systems (Madenjian et al
(1993)).

ABM techniques have been applied to the management of forests, by simulating the establishment, growth,
and death of individual trees, taking account of tree-tree interactions as they compete for resources. These
models have been used to help forest management and assess the systemic impact of environmental
changes, such as widespread deforestation.

13

13

See Liu and Ashton (1995).
18

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

18

Figure 12: Migrating animals

Figure 13: Oceanographic patterns

Source: Bjørn Christian Tørrissen (CC-BY-SA-3.0). Wildebeest on
Source: Bruno de Giusti (CC-BY-SA-2.5-IT). Fish (image

the Serengeti (image available here).

available here).

(g) Economics and Finance

Although the path less followed, ABM techniques have found a number of applications in economics and
finance. In 1957, Guy Orcutt proposed a new model which “consists of various sorts of interacting units
which receive inputs and generate outputs” (Orcutt (1957)). Orcutt was ahead of his time: just 8 of his
subsequent 500+ citations are from before 1980.

Perhaps the most famous application of ABMs in economics is Thomas Schelling's work on racial
segregation (Schelling (1969, 1971)). This demonstrated how, in a simple cellular structure with agents
following simple rules of thumb, a pattern of segregation might naturally emerge. The model’s predictions
closely matched locational patterns in real cities and communities.

In the 1980s, Robert Axelrod applied ABMs to game theory (Axelrod (1997)). These could be used to
simulate the outcome of dynamic games among interacting agents over time. For example, the optimal
behavioural response of agents was often different in repeated games. These techniques found application
in real-world settings, from trade wars to Cold Wars.

14

Through the 1990s, some Wall Street firms began using agent-based models to forecast financial variables –
for example, prepayment rates for individual mortgages, with high degrees of reliability.

15

ABMs techniques

became attractive in helping explain how even highly irrational agents might still produce efficient markets
and could explain some of the irregularities in financial markets, such as crashes and fat-tailed asset prices
distributions.

16

In the light of the crisis, interest in ABM methods has blossomed, albeit from a low base. They have been
used to study the effects of fiscal and monetary policies (Dosi et al (2015)), systemic risk (Geanakoplos et al

14
15
16

Epstein and Axtell (1996).
Geanakoplos et al. (2012).
Alfi, Cristelli, Pietronero and Zaccaria (2009).
19

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

19

(2012)) and financial market liquidity (Bookstaber (2015)). ABMs of entire economies have also begun to be
developed.

One example here is the Complexity Research Initiative for Systemic Instabilities (CRISIS), an open source
collaboration between academics, firms, and policymakers (Klimek (2015)). Another is EURACE, a large
micro-founded macroeconomic model with regional heterogeneity (Dawid (2012)). A third is the Complex
Adaptive System model, which incorporates bounded rationality and heterogeneity to reproduce business
cycles.

17

A fourth is the MINSKY model.

18

Figure 14: Segregation

Figure 15: Financial markets

Source: Esther Bubley (public domain). Racial segregation in

Source: Katrina Tuliao (CC-BY-2.0). Philippine stock market board

Georgia (image available here).

(image available here).

The Costs and Benefits of ABM

If ABMs have found relatively limited application in economics and finance, despite widespread application
elsewhere, does that matter? That depends on the potential benefits, and associated costs, of ABM
techniques in understanding economic phenomena.

Starting with costs, ABM technology has been transformed over the past decade, for two reasons. First, the
cost and speed of running these models has been revolutionised. As computing capacity has grown in line
with Moore’s Law, the numbers of interacting agents that can be simulated using ABMs has escalated. The
largest ABMs can now deal with interactions among 100 billion agents (LLNL (2013))). This is an order of
magnitude larger than the number of humans on the planet.

Second, there has been a revolution, every bit as significant, in the availability of data to callibrate these
models. This Big Data revolution is hardly unique to economics and finance, affecting pretty much every
17

Dilaver, Jump, and Levine (2016).
This is being developed by Steve Keen and Russell Standish, further details are available here:
http://www.debtdeflation.com/blogs/minsky/.
18

20

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

20

aspect of academe, business and public policy (McKinsey (2011)). Nonetheless, with economics and
finance a late-adopter of ABM technology, large-scale and more granular economic and financial data
removes an important constraint on the development of these models.

At the same time as the costs of developing and simulating ABMs have shrunk, it seems likely the benefits of
developing these models may have become larger. Inter-connections between agents have lengthened and
strengthened over recent decades, locally and globally (Haldane (2015)). This connective tissue linking
individuals and economies can take a variety of forms.

Flows of goods and services along global supply chains have never been larger. Flows of people across
borders have probably never been greater. Flows of capital across borders have certainly never been
greater. And, most striking of all, flows of information across agents and borders are occurring on an
altogether different scale than at any time in the past. All of these trends increase the importance of taking
seriously interactions between agents when modelling an economy’s dynamics.

These benefits can perhaps best be illustrated by drawing out some of the key behavioural differences
between ABM and mainstream macro-models. These differences should not be exaggerated: in practice,
ABMs lie along a spectrum with micro-founded DSGE models at one end and statistical models at the other.
But nor should they be overlooked.

(a) Emergent Behaviour

In standard macro-models, system dynamics are fully defined by the distribution of shocks to the economy
and the behavioural parameters determining how they ripple through the system. There is a classic
Frisch/Slutsky impulse-propagation mechanism determining the economy’s fortunes. If the distribution of
shocks and the parameters of the model are known and fixed, the dynamics of this system are well-defined
and predictable.

Complex systems, of which ABM are one example, do not in general have these properties. The
Frisch/Slutsky decomposition is very unlikely to be stable, if it exists at all. The reason is that a complex
system’s dynamics do not derive principally from disturbances arising outside the system but from
interactions within the system. Dynamics are endogenously, not exogenously, driven.

These feedback effects within the system may either amplify or dampen cycles. They may also give rise to
abrupt shifts or discontinuities in system behaviour if pushed beyond a critical threshold or tipping point
(Wilson and Kirman (2016)). These complex patterns are often referred to as “emergent” behaviours
because they “emerge” without any outside stimulus. And because these emergent patterns arise from
complex interactions, they are often difficult or impossible to predict.

21

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

21

To bring this to life, imagine that instead of a single wooden rocking horse, the system instead comprised a
pack of wild horses. Taking a stick to one of them will generate “emergent” behaviour. It may result in them
all staying put, one of them fleeing or all of them fleeing. And if they do all flee, this will be in a direction, and
to a destination, impossible to predict. These emergent behaviours depend crucially on behavioural
interactions within the system.

In the natural sciences, examples of these emergent behaviours are legion. They include the dynamics of
sandpiles which “self-organise” as each new grain of sand is added until a tipping point is reached and
collapse occurs (Bak, Tang and Wiesenfeld (1988)); the flocking of migrating birds and fish, whose patterns
exhibit complex, and sometimes chaotic, patterns of motion (Macy and Willer (2002)); and the dynamics of
traffic jams among cars and pedestrians, whose flows are irregular and emergent (Nagel and Paczuski
(1995)).

These emergent properties of complex systems carry important implications for model-building. In these
systems, there is a sharp disconnect between the behaviour of individual agents and the behaviour of the
system as a whole. Aggregating from the microscopic to the macroscopic is very unlikely to give sensible
insights into real world behaviour, for the same reason the behaviour of a single neutron is uninformative
about the threat of nuclear winter (Haldane (2015)). The simple aggregation of “micro-founded” models,
rather than being a virtue, may then be a cause for concern.

The emergent dynamics of these systems are likely to exhibit significant degrees of uncertainty and
ambiguity, as distinct from risk (Knight (1921), Shackle (1979)). This uncertainty is intrinsic to complex
systems, and makes forecasting and prediction in these systems extremely difficult. There is unlikely to be
any simple, stable mapping from shocks through to outcomes, from causes to consequences, from stick to
rocking horse. Indeed, in these systems you do not need any shocks to generate variability in the system.
This, too, stands in sharp contrast to existing macro-economic orthodoxy, which tends to emphasize the
identification of exogenous shocks as a key factor in understanding system dynamics.
If I had a pound for every time I had heard someone in the Bank of England say “it all depends on the
shock”, I could have long since retired. Yet in complex systems, it is simply not true. There is no simple link
between either the size or source of disturbance and its downstream impact on the economy or financial
system. Indeed, in these systems the very notion of identification or causation becomes blurred.

(b) Heuristic Behaviour

Mainstream models in macro-economics and finance tend to have a fairly sophisticated treatment of risk.
Provided the distribution of possible outcomes is reasonably well-understood, this risk can be priced and
hedged in financial markets. Saving and investment behaviour can then be analysed under the assumption
agents optimise their risk-adjusted decision-making (Haldane (2012)).
22

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

22

A world of radical uncertainty, the like of which is arises in a complex system, changes that perspective
fundamentally. Uncertainty means it may sometimes be impossible to compute future outcomes. In the
language of computer science, behavioural decisions are no longer “Turing computable” (Beinhocker (2006),
Velupillai (2000)). The relevant Euler conditions, familiar from mainstream macro-models, may not even
exist.

Faced with that uncertainty, it is often rational for agents to rely on simpler decision rules (Gigerenzer
(2015)). These are often called “rules of thumb” or heuristics. Use of such rules is sometimes thought to be
arbitrary, sub-optimal or irrational. Yet in a world of uncertainty, that is far from clear. Heuristics may be the
most robust means of making decisions in a world of uncertainty.

Rather than solving a complex inter-temporal optimisation, many consumers appear to follow simple rules of
thumb when deciding their spending (Allen and Carroll (2001)). Rather than solving a complex
mean-variance optimisation, many investors appear to invest passively or to equally-weight assets in their
portfolios (Gigerenzer (2014)). And rather than solve a complex inter-temporal trade-off, monetary policy in
practice seems to mimic simple rules of thumb (Taylor (2016)).

Some would interpret these simple decision rules as irrational, in the sense of being inconsistent with the
Euler condition from standard macro-models. But even the concept of rationally needs careful
reconsideration in an environment of radical uncertainty. Rationality can only be defined in relation to the
environment in which decisions are made – what some have called ecological rationality (Gigerenzer
(2014)). Heuristics can be the ecologically-rational response to radical uncertainty.

In ABMs, the behaviour of agents is characterised, not by Euler conditions, but by behavioural rules of
thumb. These systems also exhibit radical uncertainty. That means there is a degree of model-consistency
in ABMs – heuristics and uncertainty are mutually consistent. In that sense, the behavioural rules embedded
in ABMs are neither as irrational, nor as prone to the Lucas critique, as some critics might imply.

(c) Non-Normal Behaviour

In many standard models, the equilibrium of the system is both singular and stationary. There is a natural
and unique state of rest towards which the model converges following a disturbance. Wicksell’s rocking
horse is not a perpetual motion machine and nor does it turn somersaults. While many models of multiple
equilibria exist in economics and finance, they tend to occupy the suburbs rather than the city-centre of the
profession.

In ABMs, the equilibria which emerge are often non-stationary or multiple, sometimes both. The equilibrium
may often be an evolutionary one, the type of which often arises in ecological and biological models. The

23

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

23

dynamics around this equilibrium are also often highly non-linear, and sometimes discontinuous, with a
degree of non-linearity which is state-dependent (Taleb (2014)).

The combined effect of non-stationary, multiple equilibria and highly non-linear dynamics makes for
non-standard, and often highly non-normal, distributions for the variables in these systems. For example,
they are more likely to exhibit excess sensitivity in their fluctuations relative to fundamentals. And they may
also be subject to large dislocations or discontinuities. In consequence, they are liable to have much fatter
tails than the Gaussian distributions that often emerge from linearised, DSGE models.

(d) Matching the Dappled World
These features of ABMs – emergent behaviour, multiple equilibria, non-normalities - can be illustrated using
the original cellular automata example, Conway’s Game of Life. Take as the starting point the simple cellular
structure shown in Figure 16. Now augment that with some very simple behavioural rules of thumb
describing the evolution of the cells.

19

Then run this model forward to see what behaviour emerges among

the cells.

As the simulation shows, even a simple cellular

Figure 16: Conway’s game of life

structure following simple decision rules gives rise
to complex system dynamics. In this example, the
properties of this system are clearly “emergent” –
you would be hard-pressed to have predicted
these patterns ex-ante. They give rise to a
multiplicity of non-stationary equilibria, which
evolve through time. And the resulting distribution
of outcomes in this system is highly non-normal,
non-linear and fat-tailed.

Of course, this system is hypothetical. So it is
interesting to ask whether the behaviour exhibited
in real-world economic and financial systems is

Source: Kieff (CC-BY-SA-3.0-migrated-with-disclaimers). Bill Gosper’s
“Glider Gun” animation created by Johan Bontes and available here.

broadly consistent with the patterns from complex ABMs. One simple, reduced-form way to assess that is to
look at the statistical distribution of various economic and financial time-series for evidence of discontinuity,
non-normality and fat-tails. These are properties which, we know, exist in other natural and social systems
(Barabasi (2005), Turrell (2013)).

19

These are: Any live cell with fewer than two live neighbours dies; Any live cell with two or three live neighbours lives on to the next
generation; Any live cell with more than three live neighbours dies; Any dead cell with exactly three live neighbours becomes a live cell.
24

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

24

The short answer is yes – the properties of economic and financial systems are little different than other
social, and many natural, systems (Haldane and Nelson (2012)). To provide a few illustrations, Chart 6 looks
at the distribution of a set of economic and financial variables over a time-series dating back at least 150
years – equity prices, bond prices, GDP etc. These historical distributions can be compared with a normal
curve calibrated to the same data.

In each case, there is strong evidence of non-normality in the empirical distribution. Specifically, the tails of
the historical distribution are often significantly higher than normality would imply. For real variables like
GDP, around 18% of the data across the UK, US, Germany and Japan fall outside the ‘bell’ curve described
by a best-fit normal distribution. For financial variables like equity prices, around 10% of that data fall outside
the normal distribution. In both cases, there is evidence of out-sized dislocations in variables, which occur
with far-higher frequency than normality would imply. This evidence is consistent with, if not proof of,
complex system dynamics.

Chart 6: Long-run distributions of economic and financial variables

UK House Prices: 1846-2015, Annual

Oil Prices: 1862-2015, Annual

M4 Credit Growth: 1881-2015, Annual

CPI inflation: 1662-2015, Annual

25

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

25

World GDP growth (across the UK, US,

UK GDP growth: 1701-2015, Annual

Germany, Japan): 1871-2015, Annual

UK wage growth: 1751-2015, Annual

Equity prices: 1709-2016, Monthly

Corporate bond spreads: 1854-2016, Monthly

Dollar-Sterling Exchange rate: 1791-2016,
monthly

Source: Hills, Thomas and Dimsdale (2016); Bank calculations. Notes: For each empirical distribution f(x), a Gaussian fit to f(x) of the
(𝑥−𝜇)2

form 𝑔(𝑥) = 𝐴exp (−
) is sought in which A, 𝜇 and 𝜎 are varied and optimised using the Levenberg–Marquardt non-linear least
2𝜎2
squares fitting algorithm. Note that to achieve the best fit to the empirical data, the fitted distributions do not integrate to unity.

26

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

26

Real-World Applications of Agent-Based Models

Given that ABMs potentially better match the moments of real-world data, at least in some situations and in
some markets, and given their seeming success in other disciplines, the Bank of England has recently made
an investment in them as part of its One Bank Research Agenda (Bank of England (2015)). Let me briefly
describe two pieces of Bank research which have drawn on ABMs in an attempt to better understand two
markets and how policy might reshape dynamics in these markets.

(a) The UK Housing Market

The housing market has been one of the primary sources of financial stress in a great many countries
(Jorda, Schulerick and Taylor (2014)). Not coincidentally, this market has also been characterised by
pronounced cyclical swings. Chart 7 runs a filter through UK house price inflation in the period since 1896.
It exhibits clear cyclicality, with peak-to-trough variation often of around 20 percentage points. Mortgage
lending exhibits a similar cyclicality.

House prices, like other asset prices, also
exhibit out-sized booms and busts. Chart 6

Chart 7: Long-run UK house price growth 1846 to 2015

plots the distribution of UK house price growth

Percent year on year

since 1846. It has fat-tails, with the probability

25
20

mass of big rises or falls larger than implied by
a normal distribution. For example, the

15

probability of a 10% movement in house prices

10

in any given year is twice as large as normality

5

would imply.
0
-5

Capturing these cyclical dynamics, and fattailed properties, of the housing market is not
straightforward using aggregate models.
These models typically rely, as inputs, on a
small number of macro-economic variables,
such as incomes and interest rates. They have

-10
1846 1866 1886 1906 1926 1946 1966 1986 2006
Source: Hills, Thomas and Dimsdale (2016); Bank calculations. Notes:
The chart shows the Hodrick-Prescott trend in annual house price
growth data (where lambda=6.25). Data during WWI and WWII are
interpolated.

a mixed track record in explaining and
predicting housing market behaviour.

One reason for this poor performance may be that the housing market comprises not one but many
sub-markets - a rental market, sales market, a mortgage market etc. Moreover, there are multiple players
operating in these markets - renters, landlords, owner-occupiers, mortgage lenders and regulators – each
with distinctive characteristics, such as age, income, gearing and location.
27

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

27

It is the interaction between these multiple agents in multiple markets which shapes the dynamics of the
housing market. Aggregate models suppress these within-system interactions. The housing market model
developed at the Bank aims to unwrap and model these within-system interactions and use them to help
explain cyclical behaviour (Baptista, Farmer, Hinterschweiger, Low, Tang and Uluc (2016)).

Specifically, the model comprises households of three types:


Renters who decide whether to continue to rent or attempt to buy a house when their rental contract
ends and, if so, how much to bid;



Owner-occupiers who decide whether to sell their house and buy a new one and, if so, how much to
bid/ask for the property; and



Buy-to-let investors who decide whether to sell their rental property and/or buy a new one and, if so,
how much to bid/ask for the property. They also decide whether to rent out a property and, if so, how
much rent to charge.

The behavioural rules of thumb that households follow when making these decisions are based on factors
such as their expected rental payments, house price appreciation and mortgage cost. These households
differ not only by type, but also by characteristics such as age and income.

An important feature of the model is that it includes an explicit banking sector, itself a feature often missing
from off-the-shelf DSGE models. The banking sector provides mortgage credit to households and sets the
terms and conditions available to borrowers in the mortgage market, based on their characteristics. The
banking sector’s lending decisions are, in turn, subject to regulation by a central bank or regulator. They set
loan-to-income (LTI), loan-to-value (LTV) and interest cover ratio policies, with the objective of safeguarding
the stability of the financial system. These so-called macro-prudential policy measures are being used
increasingly by policy authorities internationally (IMF-FSB-BIS (2016)).

The various agents in the model, and their inter-linkages, are shown schematically in Figure 17.

28

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

28

Figure 17: Agents and interactions in the housing market model

Source: Baptista et al (2016).

This multi-agent model can be calibrated using micro datasets. This helps ensure agents in the model have
characteristics, and exhibit behaviours, which match those of the population at large. For example, the
distribution of loan-to-income or loan-to-value ratios on mortgages are calibrated to match the UK population
using data on over a million UK mortgages. And the impact on the sale price of a house of it remaining
unsold is calibrated to match historical housing transactions data.

One of the key benefits of the ABM approach is in providing a framework for drawing together and using, in a
consistent way, data from a range of sources to calibrate a model. For example, a variety of data sources
were used to calibrate this model, including:


Housing market data: FCA Product Sales Data, Council of Mortgage Lenders, Land Registry and
WhenFresh/Zoopla.



Household surveys: English Housing Survey, Living Cost and Food Survey, NMG Household
Survey, Wealth and Asset Survey, Survey of Residential Landlords (ARLA) and Private Landlord
Survey.

Micro-economic data such as these are essential for understanding the impact of regulatory policies – for
example, macro-prudential policies which affect the housing market. For example, the Bank has been
making use of the FCA’s Product Sales Database to get a more granular picture of the mortgage position of
households. This is a very detailed database, covering over 13 million financial transactions by UK
29

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

29

households since 2005. By combining these data with land registry data, it is also possible to build up a
regional picture of pockets of indebtedness.

Chart 8 documents the evolution of
high (more than 4.5 times income)

Chart 8: Proposition of mortgages with a loan-to-income ratio greater
than 4.5

leverage mortgages since 2008, on
a regional basis. Warmer colours
suggest a higher fraction of loans at
or above that multiple. What you
will see is a gradual heating-up of
the mortgage market over the past
few years, with a clear epicentre of
London and the South-East in the
run up to the macro-prudential
intervention made by the Bank of
England’s Financial Policy

Source: FCA Product Sales Database; Land Registry; Bank calculations. Please see

Committee (FPC) in June 2014.

the speech website for the animation.

One of the key features of an agent-based model is that it is able to generate complex housing market
dynamics, without the need for exogenous shocks. In other words, within-system interactions are sufficient
to generate booms and busts in the housing market. Cycles in house prices and in mortgage lending are, in
that sense, an “emergent” property of the model.

Chart 9 shows a simulation run of the model, looking at the dynamic behaviour of listed prices, house prices
when sold and the number of years a property is on the market. The model exhibits large cyclical swings,
which arise endogenously as a result of feedback loops in the model. Some of these feedback loops are
dampening (“negative feedback”), others amplifying (“positive feedback”).

For example, when mortgage rates fall, this boosts the affordability and the demand of housing, putting
upward pressure on house prices. This generates expectations of higher future house price inflation and a
further increase in housing demand - an amplifying loop. Ultimately, however, affordability constraints bite
and dampen house prices expectations and demand – a dampening loop.

We can use the simulated data from Chart 9 to construct distributions of house price inflation over time
(Chart 10). This simulated distribution exhibits fat-tails, if not as heavy as the historical distribution.
Nonetheless, the model goes some way towards matching the moments of the real-world housing market.

30

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

30

Chart 9: Model simulations of the housing market

Chart 10: The distribution of house prices
Percent year on year

30
25

Real house price growth

20
15

Simulation

10
5
0
-5
-10
-15
-20
-20

30

80

130

180

Number of years
Source: Baptista et al (2016). Notes: Blue is the list price index, red
the house price index and green the number of years a house is on
the market. Please see the speech website for the animation.

Source: Baptista et al (2016); Hills, Thomas and Dimsdale
(2016); Bank calculations. Notes: The blue diamonds show the
distribution of simulated house price growth for over 160 years
from the model. The red diamonds show the distribution of real
house price growth between 1847 and 2015.

This same approach can also be used to examine the impact of various macro-prudential policy measures,
whether hard limits (such as an LTV limit of 80% for all mortgage contracts) or soft limits (such as an LTI cap
for some fraction of mortgages). These policies could also be state-contingent (such as an LTV limit if credit
growth rises above a certain threshold).

As an example, we can simulate the effects of introducing a loan-to-income (LTI) limit of 3.5, where 15% of
mortgages are not bound by this limit. This simulation is similar, if not directly comparable, to the
macro-prudential intervention made by the Bank of England’s Financial Policy Committee (FPC) in
June 2014.

Chart 11 looks at the simulated impact of this policy on the distribution of loan-to-income ratios across
households, relative to a policy of no intervention. The incidence of high LTI mortgages (above 3.5)
decreases, with some clustering just below the limit. With some borrowers nudged out of riskier loans, a
greater degree of insurance is provided to households and the banking system. Another advantage of these
class of models is that they allow you to simulate the longer run impact once the second round and feedback
loops have taken effect. Chart 13 shows that the distribution of house price growth narrows under the
scenario relative to the baseline.

31

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

31

Chart 11: Simulated effect of a loan-to-income policy

Chart 12: Simulated effect on house price growth

Source: Baptista et al (2016).

Source: Baptista et al (2016).

(b) An Agent-Based Model of Financial Markets

A second ABM project looks at behaviour in financial markets (Braun-Munzinger, Liu, and Turrell (2016)). As
in the housing market, this involves complex interactions between multiple agents. And as in the housing
market, these interactions are prone to abrupt dislocations in prices, fattening the tails of the asset price
distribution. Chart 13 looks at the empirical distribution of daily corporate bond price movements over
various intervals, pre and post crisis. It is clearly fat-tailed.

The dynamics of financial markets are also

Chart 13: Distribution of corporate bond price returns

an area of active policy interest, not least in
the light of the financial crisis. During the
crisis, there were sharp swings in asset
prices and liquidity premia in many financial
markets. Since the crisis, there have been
concerns about market-makers’ willingness
to make markets, potentially impairing
liquidity. These are policy questions which
are not easily amenable to existing
asset pricing models.

Braun-Munzinger et al (2016) build a model
which seeks to capture some of the

Source: Braun-Munzinger et al (2016). Notes: The distribution function of
daily log-price returns of three periods shown against data generated by a
single model run.

interactions between market players that might give rise to these asset price patterns. In particular, the
model comprises three classes of agent: a market maker, making two-way prices in the asset; a set of
funds trading in the asset, but pursuing distinct trading strategies; and end-investors in these funds.

32

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

32

Funds are, in turn, assumed to be one of three types: value traders – who assume yields converge over time
to some equilibrium value, buying/selling when the asset is under/over-valued; momentum traders – who
follow short-term trends on the assumption they persist; and passive funds – who only trade in response to
in- and out-flows from investors. These interactions are shown schematically in Figure 18.

Figure 18: Overview of the corporate bond trading

Table 3: Fund characteristics

model

Source: Braun-Munzinger, Liu, and Turrell (2016).

Source: Morningstar and Bank of America Merrill Lynch. Notes:
The model is calibrated against empirical data including the flowperformance relationship and the distribution of sizes of openended corporate bond funds.

The model is based on, and calibrated against, the corporate bond market using micro-level data on 1,000
mutual funds. These data can be used to calibrate the size distribution of funds, their trading behaviours and
the links between their performance and in and outflows from the fund. Some of the fund characteristics are
shown in Table 3.

The interactions among these players give rise to interesting dynamics, some of which are shown
schematically in Figure 19. For example, imagine a shock to the expected loss on a bond. This reduces
demand for that bond by funds holding it and causes a re-pricing by the market-maker and momentum
selling by funds, generating a further fall in the bond’s price and in the wealth of the funds holding it.

33

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

33

Figure 19: Transmission mechanism of expected loss rate shock

Source: Braun-Munzinger et al (2016). Notes: A schematic showing the feedback loops following a shock to the value of the expected
loss rate. The colours in the feedback loop indicate the different market players; funds, the market maker, and the investor pool.

This fall in fund performance then gives rise to a second feedback loop, inducing investor withdrawals from
those funds which have under-performed, further reducing demand for the bond and amplifying the fall in its
price. It is only after some time that the influence of value investors stabilises the market.

Each individual run of the model is like hitting a wild horse once and has an unpredictable outcome. But we
can get an idea of the general behaviour of the funds by running the same scenario repeatedly – if you like,
hitting the wild horses hundreds of times and looking at their most likely response. The most likely behaviour
of funds in this model-market are oscillatory, with shocks amplified in the short run and only damped after a
period of several hundred days (Chart 14).

But by rolling the dice over and over again, we can also look at the distribution of possible outcomes, as the
average may hide extreme behaviour in the tails. For example, if the fraction of passive investors increases,
this dampens average changes in bond yields (Chart 15). But it also increases the chances of much bigger
changes in yields – the tails of the distribution fatten.

34

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

34

Chart 14: Impact of a shock to the expected loss rate

Chart 15: Distribution of outcomes after a shock to
the expected loss rate

Source: Braun-Munzinger et al (2016). Notes: What happens after
a shock to the expected loss rate; a sudden change in firms'
expected loss rate on bonds causes both short-term fluctuations in
yield, and a new, higher long-term yield. Results presented are the
median of 100 individual simulations runs, individual model runs
exhibit a range of outcomes.

Source: Braun-Munzinger et al (2016). The outcomes for
median yield over 100 trading days after a 0.36% loss rate
shock. Percentiles indicate the distribution of results taken from
250 simulation runs.

How can this model help in understanding the dynamics of real-world financial markets and the appropriate
setting of policy in these markets? Chart 13 compares the actual distribution of corporate bond prices
changes with the distribution which emerges from ABM simulations. There is a reasonable correspondence
between the two, with fatter than normal tails.
The model can be used counter-factually – for example, to assess the impact of a rise in the number of
passive or momentum traders relative to value investors. This makes for larger and longer-lived oscillations.
So too does a reduction in the market-making capacity – for example, lower market-maker inventories – as
this amplifies the impact on prices of any shock to fund demand.

One topical policy issue is whether constraints might be imposed on some funds to forestall investor
redemptions in the face of falling prices and performance. For example, US money market mutual funds
experienced such redemption runs during the course of the financial crisis. And, more recently, UK property
investment funds also exhibited run-like redemptions following the EU referendum result, which depressed
asset prices.

35

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

35

The model can be used to assess the impact of
different approaches to constraining redemption.

Chart 16: Impact of action to reduce speed with which
investor redemption requests are fulfilled

Chart 16 shows the impact on yields of varying
the redemption window, for three different rates
of investor flow. Extending the redemption
window from one day to one month would,
according to the model, have reduced amplitude
of the resulting asset price cycle by a factor of
around five.

Conclusion

In one of his most famous metaphors, Shackle
described the economy as a kaleidoscope, a
collision of colours subject to on-going, rapid
and radical change. Many of our existing
techniques for modelling and measuring the
economy invoke a rather different metaphor,

Source: Braun-Munzinger et al (2016). Notes: When the parameter
which controls the flow of money out of funds, s, is large, redemption
management can significantly reduce the magnitude of the maximum
change in yield following a large loss rate shock. The redemption
management policy splits investor redemptions over a variable number
of days with diminishing returns. The loss rate shock is 0.35%, which
is a comparable order of magnitude as the annual loss rate change
between 2007 and 2008.

with the economy a rather colourless, inanimate
rocking horse.

Both approaches have their place in making sense of the dappled economic and financial world. But, to
date, both have not been given equal billing. The global financial crisis is an opportunity to rebalance those
scales, to take uncertainty and disequilibrium seriously, to make the heterodox orthodox. A profession that
has, perhaps for too long, been shackled needs now to be Shackled.

36

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

36

References
Alfi, V, Cristelli, M, Pietronero, L, and Zaccaria, A (2009), ‘Minimal agent based model for financial markets
I’, The European Physical Journal B,67(3), 385-397
Allen, T and Carroll, C (2001), ‘Individual learning about consumption’, NBER Working Paper No 8234
An, G, Mi, Q, Dutta-Moscato, J and Y Vodovotz(2009), ‘Agent-based models in translational
systems biology’, Wiley Interdisciplinary Reviews: Systems Biology and Medicine, vol 1, no 2, pp 159–171
Ausk, B J, Gross, T and Srinivasan, S (2006), ‘An agent based model for real-time signaling induced in
osteocytic networks by mechanical stimuli’, Journal of biomechanics 3914: 2638-2646
Axelrod (1997), R M Axelrod, ‘The complexity of cooperation: Agent-based models of competition and
collaboration’, Princeton University Press
Bak, P, Tang, C and Wiesenfeld, K (1988), ‘Self-organized criticality’, Physical review A 38, no 1: 364
Bank of England (2015), ‘One Bank Research Agenda: Discussion Paper’, available at:
http://wwwbankofenglandcouk/research/Documents/onebank/discussionpdf
Baptista, R, Farmer, J D, Hinterschweiger, M, Low, K, Tang, D and Uluc, A (2016), ‘Macroprudential policy in
an agent-based model of the UK housing market’, Bank of England Staff Working Paper No 619
Barabasi, A L (2005), ‘The origin of bursts and heavy tails in human dynamics’, Nature, 435(7039), 207-211

Battiston, S, Farmer, J D, Flache, A, Garlaschelli, D, Haldane, A G, Heesterbeek, H, and Scheffer, M (2016),
‘Complexity theory and financial regulation’, Science, 351(6275), 818-819
Batty, M, Desyllas, J and Duxbury, E (2003), ‘Safety in Numbers? Modelling Crowds and Designing Control
for the Notting Hill Carnival’, Urban Studies, Vol 40
Beinhocker, E D (2006), ‘The origin of wealth: Evolution, complexity, and the radical remaking of
economics’, Harvard Business Press
Bernanke, B (2004), ‘The Great Moderation’, Speech given at the Eastern Economic Association,
Washington, available at: https://wwwfederalreservegov/boarddocs/speeches/2004/20040220/

37

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

37

Black, F and Scholes, M (1973), ‘The Pricing of Options and Corporate Liabilities’, Journal of Political
Economy, Vol 81
Bookstaber, R and Paddrik, M E (2015), ‘An Agent-based Model for Crisis Liquidity Dynamics’, OFR WP:
15-18
Braun-Munzinger, K, Liu, Z and Turrell, A (2016), ‘An agent-based model of dynamics in corporate bond
trading’, Bank of England Staff Working Paper No 592, available at
http://www.bankofengland.co.uk/research/Pages/workingpapers/2016/swp592.aspx
Brooks, H, DeKeyser, T, Jasckot, D, Sibert, D, Sledd, R, Stilwell, W, Scherer, W (2004), ‘Using agent-based
simulation to reduce collateral damage during military operations’, Proceedings of the 2004 Systems and
Information Engineering Design Symposium
Brush, S, Sahlin, H and Teller,E (1966), ‘Monte Carlo study of a one-component plasma i’,The Journal
of Chemical Physics, vol 45, no 6, pp 2102–2118
Buchanan, M (2014), ‘Forecast: What Physics, Meteorology, and the Natural Sciences Can Teach Us About
Economics’, Bloomsbury USA
Bulanov, S V and Khoroshkov, V S (2002), ‘Feasibility of using laser ion accelerators in proton therapy’,
Plasma Physics Reports 285: 453-456

Burgess, S, Fernandez-Corugedo, E, Groth, C, Harrison, R, Monti, F, Theodoridis, K and Waldron, M (2013),
‘The Bank of England's forecasting platform: COMPASS, MAPS, EASE and the suite of models’, Bank of
England Working Paper No 471, available at:
http://wwwbankofenglandcouk/research/Pages/workingpapers/2013/wp471aspx
Cartwright, N (1999), ‘The Dappled World: A Study of the Boundaries of Science’, Cambridge University
Press
Champagne, L E (2003), ‘Agent models ii: Bay of biscay: extensions into modern military issues’,
proceedings of the 35th conference on Winter simulation: driving innovation, pp 1004–1012
Chen, B, and Cheng, H H (2010), ‘A review of the applications of agent technology in traffic and
transportation systems’, Intelligent Transportation Systems, IEEE Transactions on 112,: 485-497
Chen, J and Churchill, S (1963), ‘Radiant heat transfer in packed beds’, AIChE Journal 9(1): 35-41

38

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

38

Conway, J (1970), ‘The game of life’, Scientific American 2234: 4
Coyle, D and Haldane, A G (2014), ‘Financial crash: what’s wrong with economics?’, Prospect Magazine,
available at: http://wwwprospectmagazinecouk/economics-and-finance/financial-crash-whats-wrong-witheconomics
Davis, M, Efstathiou, G, Frenk, C S and White, S D (1985), ‘The evolution of large-scale struc-ture
in a universe dominated by cold dark matter’, The Astrophysical Journal, vol 292, pp 371–394
Dawid, H et al (2012), ‘The eurace@unibi model: An agent-based macroeconomic model for economic policy
analysis’
DeAngelis, D L, Cox, D and Coutant, C (1980), ‘Cannibalism and size dispersal in young-of-theyear largemouth bass: experiment and model’, Ecological Modelling, vol 8, pp 133–148
Debreu, G (1974), ‘Excess demand functions’, Journal of mathematical economics, vol 1, no 1,
pp 15–21
Degli, A et al (2008), ‘Mitigation measures for pandemic influenza in Italy: an individual based model
considering different scenarios’, PLoS One 33: e1790
Dilaver, O, Jump, R and Levine, P (2016), ‘Agent-based Macroeconomics and Dynamic Stochastic General
Equilibrium Models: Where Do We Go from Here?’, Discussion Papers in Economics 1: 16
Dosi, G et al (2015), ‘Fiscal and monetary policies in complex evolving economies’, Journal of
Economic Dynamics and Control 52: 166-189
Epstein, J M and Axtell, R (1996), ‘Growing artificial societies: social science from the bottom up’, Brookings
Institution Press
Fagiolo, G and Roventini, A (2012), ‘Macroeconomic policy in dsge and agent-based models’,
Revue de l’OFCE, vol 124, no 5, pp 67–116
Fagnant, D J and Kockelman, K M, (2014), ‘The travel and environmental implications of shared autonomous
vehicles, using agent-based model scenarios’, Transportation Research Part C: Emerging Technologies 40:
1-13
Fourcade, M, Ollion, E and Algan, Y (2015), ‘The superiority of economists’, Journal of Economic
Perspectives, Vol 29

39

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

39

Gali, J (2006), ‘Monetary policy, inflation and the business cycles’, Princeton University Press
Geanakoplos, J et al (2012), ‘Getting at systemic risk via an agent-based model of the housing market’,
The American Economic Review 1023: 53-58
Gigerenzer, G (2014), ‘Risk Savvy: How to make good decisions’, Allen Lane, ISBN: 978-1846144745
Gigerenzer, G (2015), ‘Risk Savvy: How To Make Good Decisions’, ISBN: 978-0241954614
Grimm, V (1999), ‘Ten years of individual-based modelling in ecology: what have we learned and
what could we learn in the future?’, Ecological modelling, vol 115, no 2, pp 129–148
Haldane, A G (2012), ‘Financial arms races’, available at:
http://wwwbankofenglandcouk/publications/Documents/speeches/2012/speech565pdf
Haldane, A G (2015), ‘On microscopes and Telescopes’, available at:
http://wwwbankofenglandcouk/publications/Pages/speeches/2015/812aspx
Haldane, A G and Nelson, B (2012), ‘Tails of the unexpected’, speech given at The Credit Crisis Five Years
On: Unpacking the Crisis conference held at the University of Edinburgh Business School
Harari, Y (2015), ‘Sapiens: A Brief History of Humankind’, Vintage Publishers, ISBN: 978-0099590088
Heath, M R and Gallego, A (1998), ‘Bio-physical modelling of the early life stages of haddock,
Melanogrammus aeglefinus, in the North Sea’, Fisheries Oceanography 72: 110-125
Hill, R R, Champagne, L E and Price, J C (2004), ‘Using agent-based simulation and game theory to
examine the WWII Bay of Biscay U-boat campaign’, The Journal of Defense Modeling and Simulation:
Applications, Methodology, Technology, vol 1, no 2, pp 99– 109
Hills, S, Thomas, R and Dimsdale, N (2016), ‘Three Centuries of Data - Version 23’, available here:
http://wwwbankofenglandcouk/research/Pages/onebank/threecenturiesaspx
IMF-FSB-BIS (2016), ‘Elements of effective macro-prudential policies’, available at:
https://www.imf.org/en/News/Articles/2016/08/30/PR16386-IMF-FSB-BIS-publish-Elements-of-EffectiveMacroprudential-Policies
Jones, S S and Evans R S (2008), ‘An agent based simulation tool for scheduling emergency department
physicians’, AMIA Annual Symposium Proceedings Vol 2008 American Medical Informatics Association
40

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

40

Jorda, O, Schulerick, M, Taylor, A (2014), ‘The Great Mortgaging: Housing Finance, Crises, and Business
Cycles’, NBER Working Paper No 20501
Karr, J R et al (2012), ‘A whole-cell computational model predicts phenotype from genotype’, Cell,
vol 150, no 2, pp 389–401
Keynes, J M (1936), ‘The General Theory of Employment, Interest and Money’, Palgrave Macmillian
King, M (2016), ‘The end of Alchemy: Money, Banking and the Future of the Global Economy’, Little, Brown
Publishers
Kirman, A P (1992), ‘Whom or what does the representative individual represent?’, The Journal of
Economic Perspectives, pp 117–136
Klimek, P et al (2015), ‘To bail-out or to bail-in? Answers from an agent-based model’, Journal of Economic
Dynamics and Control 50: 144-154
Knight, F (1921), ‘Risk, uncertainty and profit’, Hart, Schaffner and Marx; Houghton Mifflin Co
Lawrence Livermore National Laboratory LLNL (2013), ‘Record simulations conducted on Lawrence
Livermore supercomputer’, Web: https://wwwllnlgov/news/record-simulations-conducted-lawrence-livermoresupercomputer
Liu, J, and Ashton, P (1995), ‘Individual-based simulation models for forest succession and management’,
Forest Ecology and Management 731: 157-175
Lucas, R (1976), ‘Econometric Policy Evaluation: A Critique’, In Brunner, K; Meltzer, A The Phillips Curve
and Labor Markets Carnegie-Rochester Conference Series on Public Policy 1 New York: American Elsevier
pp 19–46 ISBN 0-444-11007-0
Lucas, R (2009), ‘In defence of the dismal science’, article in The Economist, available at:
http://wwweconomistcom/node/14165405
Macy, M W, and Willer, R (2002), ‘From factors to actors: Computational sociology and agent-based
modeling’, Annual review of sociology, 143-166
Madenjian, C P et al (1993), ‘Accumulation of PCBs by lake trout (Salvelinus namaycush),: an
individual-based model approach’, Canadian Journal of Fisheries and Aquatic Sciences 501: 97-109

41

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

41

Manheimer, W M, Lampe, M, and Joyce, G (1997), ‘Langevin representation of Coulomb collisions in PIC
simulations’, Journal of Computational Physics,138(2), 563-584
Mansury, Y et al (2002), ‘Emerging patterns in tumor systems: simulating the dynamics of multicellular
clusters with an agent-based spatial agglomeration model’, Journal of Theoretical Biology 2193: 343-370
McKinsey (2011), ‘Big Data: The next frontier for innovation, competition and productivity’, available at:
http://www.mckinsey.com/business-functions/digital-mckinsey/our-insights/big-data-the-next-frontier-forinnovation
Metropolis, N (1987), ‘The beginning of the monte carlo method’, Los Alamos Science, vol 15, no 584,
pp 125–130
Metropolis, N and Ulam, S (1949), ‘The monte carlo method’, Journal of the American statistical
association, vol 44, no 247, pp 335–341
Mirowski, P (1989), ‘More Heat than Light - Economics as Social Physics, Physics as Nature's Economics’,
Cambridge University Press

Nagel, K, and Paczuski, M (1995), Emergent traffic jams Physical Review E,51(4), 2909
Orcutt, G H (1957), ‘A new type of socio-economic system’, The review of economics and statistics, pp
116–123
Pechoucek, M, and Sislak, D (2009), ‘Agent-based approach to free-flight planning, control, and simulation’,
Intelligent Systems, IEEE 241: 14-17
Pelechano, N, Allbeck, J M and Badler, N I (2007), ‘Controlling individual agents in high-density crowd
simulation’, Proceedings of the 2007 ACM SIGGRAPH/Eurographics symposium on Computer animation
Eurographics Association
Penrose, R (1999), ‘The emperor’s new mind: Concerning computers, minds, and the laws of physics’,
Oxford University Press
Popper, K (1934), ‘Logik der Forschung’, later translated as ‘The Logic of Scientific Discovery’, in English in
1959
Rauh, J, Schenk, T A and Schrödl, D (2012), ‘The simulated consumer-An agent-based approach to
shopping behaviour’, Erdkunde: 13-25
42

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

42

Romer, P (2016), ‘The trouble with macroeconomics’, available at: https://paulromernet/wpcontent/uploads/2016/09/WP-Troublepdf
Rosato, A et al (1987), ‘Why the brazil nuts are on top: Size segregation of particulate matter by
shaking’, Physical Review Letters, vol 58, no 10, p 1038
Sanchez, S M and Lucas, T W (2002), ‘Exploring the world of agent-based simulations: simple
models, complex analyses: exploring the world of agent-based simulations: simple models,
complex analyses’, in Proceedings of the 34th conference on Winter simulation: exploring new frontiers,
pp 116–126
Sargent, T (2001), ‘The Conquest of American Inflation’, Princeton University Press
Sarkar, P (2000), ‘A brief history of cellular automata’, ACM Computing Surveys (CSUR), vol 32, no 1,
pp 80–107
Schelling, T C (1969), ‘Models of segregation’, The American Economic Review, pp 488–493
Schelling, T C (1971), ‘Dynamic models of segregation’, Journal of mathematical sociology, vol 1, no 2,
pp 143–186
Seow, K T, and Lee, D H (2010), ‘Performance of multiagent taxi dispatch on extended-runtime taxi
availability: A simulation study’, Intelligent Transportation Systems, IEEE Transactions on 111: 231-236
Shackle, G L S (1949), ‘Expectations in Economics’, Cambridge University Press
Shackle, G L S (1972), ‘Epistemics and Economics’, ISBN: 978-1560005582
Shackle, G L S (1979), ‘Imagination and the Nature of Choice’, Edinburgh University Press
Smets, F and Wouters, R (2003), ‘An Estimated Dynamic Stochastic General Equilibrium Model of the Euro
Area’, Journal of the European Economic Association vol 1(5)
Taleb, N (2014), ‘Antifragile: Things That Gain from Disorder’, Random House Trade Paperbacks
Taylor, J (2016), ‘Policy stability and economic growth: Lessons from the Great Recession’, Institute of
Economic Affairs

43

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

43

Thorne, B C, Bailey, A M and Peirce, S M (2007), ‘Combining experiments with multi-cell agentbased modeling to study biological tissue patterning’, Briefings in bioinformatics, vol 8, no 4, pp
245–257
Tomita, M et al (1999), ‘E-cell: software environment for whole-cell simulation’, Bioinformatics, vol
15, no 1, pp 72–84
Turrell, A (2013), ‘Processes driving non-Maxwellian distributions in high energy density plasmas’ PhD
thesis, Imperial College London
Turrell, A et al (2015), ‘Ultrafast collisional ion heating by electrostatic shocks’, Nature communications,
vol 6
Van Noordan, R (2015), ‘Interdisciplinary research by the numbers’, Nature vol 525, available at:
http://wwwnaturecom/news/interdisciplinary-research-by-the-numbers-118349
Velupillai, K (2000), ‘Computable economics: the Arne Ryde memorial lectures’, vol 5 Oxford University
Press
Von Neumann, J (1951), ‘The general and logical theory of automata’, Cerebral mechanisms in behavior,
pp 1–41
Von Neumann, J et al (1966), ‘Theory of self-reproducing automata’, IEEE Transactions on Neural
Networks, vol 5, no 1, pp 3–14
Walras, L (1874), ‘Éléments d'économie politique pure, ou théorie de la richesse sociale (Elements of Pure
Economics, or the theory of social wealth, transl W Jaffé)’
Werner, F E et al (2001), ‘Spatially-explicit individual based modeling of marine populations: a
review of the advances in the 1990s’, Sarsia, vol 86, no 6, pp 411–421
Wicksell, K (1918), Wicksell’s metaphor appears in a footnote to a review of Karl Petander’s ‘Goda och
darliga tider’, published in Ekonomisk Tidskrift
Wilson, D and Kirman, A (2016), ‘Complexity and Evolution’, MIT Press, ISBN: 978-0262337700
Woodcock, A et al (1988), ‘Cellular automata: a new method for battlefield simulation’, Signal, vol 42,
pp 41–50

44

All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

44

